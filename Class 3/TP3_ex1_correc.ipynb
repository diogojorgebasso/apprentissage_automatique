{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction à l'apprentissage automatique: TP3 - Exercice 1 - <font color=red> CORRECTION </font>\n",
    "\n",
    "<br>\n",
    "\n",
    "## Détection de spam\n",
    "\n",
    "<br>\n",
    "\n",
    "Dans ce TP, nous allons entraîner des classifieurs pour décider si un mail est un spam ou non.\n",
    "\n",
    "<br>\n",
    "\n",
    "Tout d'abord, quelques indications sur l'utilisation des méthodes d'apprentissage de `scikit-learn`.\n",
    "\n",
    "Les méthodes d'apprentissage supervisé de `scikit-learn` permettent de définir un objet, doté de différents attributs et méthodes, dont `cross_val_score` (pour calculer un score de validation croisée), `fit` (pour procéder à l'apprentissage), `predict` (pour prédire les classes des éléments d'une base de test), ou `score` pour calculer la proportion d'observations bien classées dans la base de test, sur laquelle on peut comparer la classe prédite à la \"vraie\" classe.\n",
    "\n",
    "Ci-dessous, un exemple d'utilisation de la classification au plus proche voisin, dans un scénario où on suppose disposer d'une base d'apprentissage $(X_{train},y_{train})$, et d'une base de test $X_{test}$ pour laquelle on connaît $y_{test}$, de manière à valider l'apprentissage sur la base de test. Si on veut changer de classifieur, il suffit d'utiliser un autre constructeur que `neighbors.KNeighborsClassifier` et de passer les paramètres adéquats.\n",
    "\n",
    "```python\n",
    "# (le code suivant ne peut pas être exécuté \"tel quel\"...)\n",
    "\n",
    "# classifieur au plus proche voisin (on peut changer le paramètre n_neighbors):\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)  \n",
    "\n",
    "# calcul d'un score moyen de validation croisée \"à 5 plis\" sur (X_train,y_train)\n",
    "scores = cross_val_score(knn,X_train,y_train,cv=5)\n",
    "print(\"score moyen de validation croisée: %0.3f (+/- %0.3f)\" % (scores.mean(),2*scores.std()))\n",
    "\n",
    "# la prédiction d'une nouvelle observation consistera à chercher le p.p.v. dans X_train, \n",
    "# et à associer la classe de ce p.p.v., donnée par y_train:\n",
    "knn.fit(X_train,y_train)  \n",
    "# Remarque: il n'y a pas d'apprentissage à proprement parler pour les p.p.v., \n",
    "# il s'agit juste de préciser la base dans laquelle seront cherchés les plus proches voisins\n",
    "\n",
    "# on stocke dans y_pred les classes prédites sur un ensemble de test X_test:\n",
    "y_pred = knn.predict(X_test)  \n",
    "\n",
    "# calcul d'un score lorsqu'on connaît les vraies classes des observations de X_test: \n",
    "# (proportion d'observations pour lesquelles y_test==y_pred)\n",
    "score = knn.score(X_test,y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Préliminaires\n",
    "\n",
    "<br>\n",
    "\n",
    "Commençons par charger les bibliothèques utiles au TD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn import neighbors, linear_model, naive_bayes, metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, on charge les données: récupérez au préalable le fichier `spambase.data` disponible sur le [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Spambase) (cliquez sur \"Download\"). La description complète de la base est dans le fichier `spambase.name`, à ouvrir avec un éditeur de texte.\n",
    "\n",
    "<br>\n",
    "\n",
    "La cellule suivante charge les données. On forme une base d'entraînement avec 80% des données (choix aléatoire), et on garde 20% des données pour faire une base de test. Dans la cellule suivante, on fixe la graîne du générateur aléatoire (`random_state=1`, la valeur est arbitraire) de manière à ce que l'on ait tous les mêmes résultats afin de faciliter la comparaison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "    \n",
    "_Remarque_ : les générateurs pseudo-aléatoires sont généralement des générateurs congruentiels linéaires. Voir: https://fr.wikipedia.org/wiki/G%C3%A9n%C3%A9rateur_de_nombres_pseudo-al%C3%A9atoires\n",
    "\n",
    "Ils fournissent une séquence déterministe à partir d'un état initial donné (graine du générateur). Généralement, on fixe l'état initial `random_state` à partir du temps CPU, de manière à générer différentes séquences à chaque utilisation. De cette manière, chaque exécution (sur votre machine ou la machine d'un camarade) fournirait un résultat différent.\n",
    "\n",
    "Ici, on fixe tous `random_state` à la même valeur, de manière à tous avoir la même répartition train/test, et pouvoir comparer nos résultats. C'est une astuce très utilisée en phase de déboguage. Attention néanmoins à ne pas en abuser, il y a des situations où il est crucial de ne pas garder la même initialisation à chaque exécution. Par exemple, dans les algorithmes d'optimisation stochastiques (par exemple le gradient stochastique que l'on verra dans le cours sur les réseaux de neurones), il ne faut pas fixer l'état initial a priori. Dans ce dernier cas, ce qui nous intéresserait est le comportement en moyenne de l'optimisation, pas le comportement pour un choix donné d'état initial.\n",
    "    \n",
    "Une remarque cependant: attention, [`random_state` dépend de l'OS](https://github.com/lmcinnes/umap/issues/153), vous pouvez avoir des résultats différents de vos camarades sous Linux/MacOS/Windows avec la même valeur d'état initial... \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('spambase.data', delimiter=',')\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[:,:-1], data[:,-1], test_size=0.2, random_state=1)\n",
    "# pour vérifier que les données sont bien chargées:\n",
    "print(\"dataset:\")\n",
    "print(data)  \n",
    "print(\"\\nTOTAL - nombre d'observations, nombre de caractéristiques:\")\n",
    "print(data.shape)\n",
    "print(\"\\nAPPRENTISSAGE - nombre d'observations, nombre de caractéristiques:\")\n",
    "print(X_train.shape)\n",
    "print(\"\\nAPPRENTISSAGE - nombre de labels associés aux obervations:\")\n",
    "print(y_train.shape)\n",
    "print(\"\\nTEST - nombre d'observations, nombre de caractéristiques:\")\n",
    "print(X_test.shape)\n",
    "print(\"\\nTEST - nombre de labels associés aux obervations:\")\n",
    "print(y_test.shape)\n",
    "print(\"\\nobservations, base d'apprentissage:\")\n",
    "print(X_train)\n",
    "print(\"\\nlabels associés, base d'apprentissage:\")\n",
    "print(y_train)\n",
    "print(\"\\nproportion de spams dans la base d'apprentissage:\")\n",
    "print(np.mean(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 1__. A partir de la description de la base de données, justifiez la manière employée pour charger les données en `X` (observations) et `y` (labels). Quelles sont les caractéristiques des observations, les labels, et quel est le rapport avec le problème initial?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "  \n",
    "Les caractéristiques sont séparées par le délimiteur \",\" (ouvrir le fichier avec un éditeur de texte pour le vérifier).\n",
    "    \n",
    "D'après la description du fichier, chaque ligne est une observation (la description d'un mail) et regroupe 58 caractéristique, la dernière étant 1 si le message est un spam, 0 sinon.\n",
    "\n",
    "Les 57 premières caractéristiques forment donc un descripteur `x`, et `y` est bien l'étiquette dans ce problème d'apprentissage supervisé.\n",
    "     \n",
    "La description des caractéristiques est donnée par `spambase.names` (voir aussi `spambase.DOCUMENTATION`) dans l'archive sur la page web de l'UCI. La méthode pour décider des mots dont on calculera les statistiques n'est pas précisée. On pourrait s'inspirer des méthodes de quantification par clustering et descripteurs TF-IDF vues au TP2 pour sélectionner le \"sac de mots\" sur lequel on calculerait des statistiques.\n",
    "    \n",
    "Chaque mail est donc décrit par un vecteur de ${\\mathbb R}^{57}$, il s'agit d'un problème de classification biclasse (classe 0: non-spam; classe 1: spam).\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remarque importante__: lorsqu'on teste des classifieurs, il est important de comparer les scores de classification obtenus à celui d'un \"dummy classifier\" (un classifieur fictif): un classifieur qui fait une prévision sans tenir compte des observations. Par exemple, ici un classifieur qui classerait toute observation comme \"non spam\" aurait raison dans presque 60% des cas. On espère donc que les classifieurs réels soient meilleurs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification aux plus proches voisins\n",
    "\n",
    "<br>\n",
    "\n",
    "Mettez en oeuvre les classifications au plus proche voisin et aux 5 plus proches voisins. Vous calculerez le score moyen de validation croisée à 5 plis sur la base d'apprentissage ainsi que le score obtenu sur la base de test. Vous vous inspirerez du code détaillé en introduction. \n",
    "\n",
    "__Question 2__. Quelle est la métrique utilisée pour déterminer les plus proches voisins? Quel est ce \"score\" calculé exactement? Quel lien entre score de validation croisée et score sur la base de test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification 1-ppv:\")\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)  \n",
    "scores_knn = cross_val_score(knn,X_train,y_train,cv=5)  \n",
    "print(\"score moyen de validation croisée: %0.3f (+/- %0.3f)\" % (scores_knn.mean(),2*scores_knn.std()))\n",
    "knn.fit(X_train,y_train)  # (il n'y a pas d'apprentissage à proprement parler pour les p.p.v.)\n",
    "y_pred_knn = knn.predict(X_test)  \n",
    "score_knn = knn.score(X_test,y_test)\n",
    "print(\"score sur la base de test: %0.3f\" %score_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification 5-ppv:\")\n",
    "knn5 = neighbors.KNeighborsClassifier(n_neighbors=5)  \n",
    "scores_knn5 = cross_val_score(knn5,X_train,y_train,cv=5)\n",
    "print(\"score moyen de validation croisée: %0.3f (+/- %0.3f)\" % (scores_knn5.mean(),2*scores_knn5.std()))\n",
    "knn5.fit(X_train,y_train) \n",
    "y_pred_knn5 = knn5.predict(X_test)  \n",
    "score_knn5 = knn5.score(X_test,y_test)\n",
    "print(\"score sur la base de test: %0.3f\" %score_knn5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "\n",
    "\n",
    "D'après \n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "la distance euclidienne (entre vecteurs de ${\\mathbb R}^{57}$ ici) est utilisée par défaut pour déterminer les plus proches voisins.\n",
    "\n",
    "D'après la documentation:\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n",
    "le score est donné par le \"default scorer\" du classifieur, qui est d'après:\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "la \"mean accuracy\" (précision moyenne), c'est à dire ici la proportion d'observations correctement classées.\n",
    "\n",
    "Score de validation croisée et score sur la base de test ont des valeurs proches, ce qui est normal si la base de test est assez grande et bien représentative de la base d'apprentissage. Voir cours.\n",
    "\n",
    "Attention, le score de validation croisée (cf cours séance 2) est obtenu comme moyenne des scores des classifieurs pour lesquels l'apprentissage est fait, tour à tour, sur 4/5 de la base d'apprentissage, le score étant calculé sur le dernier 1/5. Par contre, le score sur la base de test est celui d'un classifieur entraîné sur l'intégralité de la base d'apprentissage.\n",
    "\n",
    "_Remarque_ : si on fait `print(scores_knn)` on affiche les scores de classifcation sur les 5 plis, qui servent ensuite à calculer le score moyen et son écart-type.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 3__. Pourquoi la métrique utilisée n'est-elle pas adaptée aux observations ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "    \n",
    "La norme euclidienne donne le même poids à chaque caractéristique. Or, les caractéristiques ne varient pas dans le même intervalle: les 54 premières caractéristiques sont des pourcentages (entre 0 et 100), ensuite les caractéristiques sont des longueurs de chaînes de caractères. Une variation de 1% dans une caractéristique n'aura donc pas le même impact sur la norme selon la nature de la caractéristique.\n",
    "    \n",
    "Par ailleurs, de manière générale on a intérêt à travailler avec des caractéristiques dans une même gamme de valeur pour des questions de stabilité numérique des algorithmes d'estimation.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 4__. Pré-traitez les données par standardisation, comme expliqué ici sur [la documentation scikit-learn](https://scikit-learn.org/stable/modules/preprocessing.html) (utilisez `StandardScaler` de manière à appliquer la même normalisation sur la base d'apprentissage et sur la base de test, c'est important), puis recalculez les scores des deux classifieurs précédents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_standard = scaler.transform(X_train)\n",
    "X_test_standard = scaler.transform(X_test)\n",
    "\n",
    "print(\"Classification 1-ppv après normalisation:\")\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)  \n",
    "scores_knn = cross_val_score(knn,X_train_standard,y_train,cv=5)  \n",
    "print(\"score moyen de validation croisée: %0.3f (+/- %0.3f)\" % (scores_knn.mean(),2*scores_knn.std()))\n",
    "knn.fit(X_train_standard,y_train) \n",
    "y_pred_knn = knn.predict(X_test_standard)  \n",
    "score_knn = knn.score(X_test_standard,y_test)\n",
    "print(\"score sur la base de test: %0.3f\" %score_knn)\n",
    "\n",
    "print(\"\\nClassification 5-ppv après normalisation:\")\n",
    "knn5 = neighbors.KNeighborsClassifier(n_neighbors=5)  \n",
    "scores_knn5 = cross_val_score(knn5,X_train_standard,y_train,cv=5)\n",
    "print(\"score moyen de validation croisée: %0.3f (+/- %0.3f)\" % (scores_knn5.mean(),2*scores_knn5.std()))\n",
    "knn5.fit(X_train_standard,y_train) \n",
    "y_pred_knn5 = knn5.predict(X_test_standard)  \n",
    "score_knn5 = knn5.score(X_test_standard,y_test)\n",
    "print(\"score sur la base de test: %0.3f\" %score_knn5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "    \n",
    "On voit que la normalisation améliore grandement le score de classification. On ne peut pas distinguer les 1-ppv et 5-ppv, qui ont un score similaire.    \n",
    "\n",
    "__Rappel__: la base test ne doit servir qu'à vérifier la performance des classifieurs, et elle ne doit pas intervenir dans l'entraînement du classifieur. On calcule donc moyenne et écart-type uniquement sur la base d'apprentissage (et pas sur la base d'apprentissage ET la base test), puis on applique la même normalisation sur les bases d'apprentissage et de test: l'objectif est de pouvoir mesurer la performance sur la base test du classifieur entraîné sur la base d'apprentissage normalisée.\n",
    "    \n",
    "__Attention à ne pas normaliser les `y`__: ils codent la classe en 0 ou 1, cela n'a pas de sens de chercher à les normaliser.\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classifieur naïf de Bayes gaussien et classifieur de la régression logistique\n",
    "\n",
    "<br>\n",
    "\n",
    "__Question 5__. On __admettra__ que le classifieur naïf de Bayes gaussien ne nécessite pas de standardisation préalable des données. La démonstration figurera dans la correction. (vous pouvez vérifier en essayant avec ou sans normalisation que la normalisation joue tout de même un faible rôle: elle a sans doute une influence sur le comportement de l'algorithme d'estimation des paramètres)\n",
    "\n",
    "Mettez en oeuvre le classifieur naïf de Bayes gaussien (lisez le début de la [documentation](https://scikit-learn.org/stable/modules/naive_bayes.html) où vous retrouverez le contenu du cours, puis la syntaxe de `GaussianNB` [ici](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "    \n",
    "__Justification de l'inutilité de standardiser les données avec GNB (complément d'information)__\n",
    "\n",
    "Dans le classifieur GNB, on modélise les distributions conditionnelles des composantes comme des gaussiennes:\n",
    "$$ p(x_i| C_k) = \\frac{1}{\\sqrt{2\\pi}\\sigma_{ik}} e^{-|x_i-\\mu_{ik}|^2/(2\\sigma_{ik}^2)}$$\n",
    "\n",
    "Ensuite, on classe une observation $(x_1,x_2,\\dots,x_{57})$ dans la classe 0 si:\n",
    "$$ p(C_0) \\prod_{i=1}^{57} p(x_i| C_0) > p(C_1) \\prod_{i=1}^{57} p(x_i| C_1)$$\n",
    "soit:\n",
    "$$ p(C_0) \\frac{1}{(2\\pi)^{57/2}\\sigma_{0}} e^{-\\frac12 \\sum_{i=1}^{57} ((x_i-\\mu_{i0})/\\sigma_{i0})^2} > \n",
    "p(C_1) \\frac{1}{(2\\pi)^{57/2}\\sigma_{1}} e^{-\\frac12 \\sum_{i=1}^{57} ((x_i-\\mu_{i1})/\\sigma_{i1})^2}$$\n",
    "où $\\sigma_k = \\prod_{i=1}^{57} \\sigma_{ik}$\n",
    "    \n",
    "En passant au logarithme:\n",
    "$$ \\log(p(C_0)) - \\log(\\sigma_0) -\\frac12 \\sum_{i=1}^{57} \\frac{(x_i-\\mu_{i0})^2}{\\sigma_{i0}^2} > \\log(p(C_1)) - \\log(\\sigma_1) -\\frac12 \\sum_{i=1}^{57} \\frac{(x_i-\\mu_{i1})^2}{\\sigma_{i1}^2} \\qquad \\text{(règle R1)}$$\n",
    "\n",
    "    \n",
    "Supposons à présent qu'on travaille sur les données normalisées. Chaque caractéristique est transformée selon $\\tilde{x}_i = (x_i - m_i)/s_i$ où $m_i$ et $s_i$ sont la moyenne et l'écart-type empiriques de la $i$-ème caractéristique.    \n",
    "\n",
    "Sur les données normalisées, la règle de décision fournie par GNB serait de classer une observation normalisée $(\\tilde{x}_1,\\tilde{x}_2,\\dots,\\tilde{x}_{57})$ dans la classe 0 si:\n",
    "$$\\log(p(C_0)) - \\log(\\tilde{\\sigma}_0) -\\frac12 \\sum_{i=1}^{57} \\frac{(\\tilde{x}_i-\\tilde{\\mu}_{i0})^2}{\\tilde{\\sigma}_{i0}^2} > \\log(p(C_1)) - \\log(\\tilde{\\sigma}_1) -\\frac12 \\sum_{i=1}^{57} \\frac{(\\tilde{x}_i-\\tilde{\\mu}_{i1})^2}{\\tilde{\\sigma}_{i1}^2} \\qquad \\text{(règle R2)}$$\n",
    "c'est-à-dire (en remplaçant les $\\tilde{x}_i$ par $(x_i - m_i)/s_i$), si:\n",
    "$$\\log(p(C_0)) - \\log(\\tilde{\\sigma}_0) -\\frac12 \\sum_{i=1}^{57} \\frac{(x_i-m_i-\\tilde{\\mu}_{i0}s_i)^2}{\\tilde{\\sigma}_{i0}^2s_i^2} > \\log(p(C_1)) - \\log(\\tilde{\\sigma}_1) -\\frac12 \\sum_{i=1}^{57} \\frac{(x_i-m_i-\\tilde{\\mu}_{i1}s_i)^2}{\\tilde{\\sigma}_{i1}^2s_i^2}$$\n",
    "    \n",
    "Maintenant, $\\tilde{\\mu}_{i0}$ est la moyenne empirique de la i-ème caractéristique des observations normalisées appartenant à la classe 0. Ainsi, $\\tilde{\\mu}_{i0} = (\\mu_{i0}-m_i)/s_i$ (pareil pour la classe 1).\n",
    "    \n",
    "De même,  $\\tilde{\\sigma}_{i0}^2$ est la variance empirique de la i-ème caractéristique des observations normalisées appartenant à la classe 0. Ainsi, $\\tilde{\\sigma}_{i0}^2 = \\sigma_{i0}^2/s_i^2$ (pareil pour la classe 1).\n",
    "    \n",
    "L'inégalité précédente s'écrit donc:\n",
    "$$\\log(p(C_0)) - \\log(\\tilde{\\sigma}_0) -\\frac12 \\sum_{i=1}^{57} \\frac{(x_i-\\mu_{i0})^2}{\\sigma_{i0}^2} > \\log(p(C_1)) - \\log(\\tilde{\\sigma}_1) -\\frac12 \\sum_{i=1}^{57} \\frac{(x_i-\\mu_{i1})^2}{\\sigma_{i1}^2}$$\n",
    "   \n",
    "Enfin, $\\log(\\tilde{\\sigma}_0) =  \\log\\left( \\prod_{i=1}^{57} \\tilde{\\sigma}_{i0}\\right) =  \\log(\\sigma_0) - \\log\\left( \\prod_{i=1}^{57} s_i^2 \\right)$\n",
    "\n",
    "Donc:\n",
    "$$\\log(p(C_0)) - \\log(\\sigma_0) -\\frac12 \\sum_{i=1}^{57} \\frac{(x_i-\\mu_{i0})^2}{\\sigma_{i0}^2} > \\log(p(C_1)) - \\log(\\sigma_1) -\\frac12 \\sum_{i=1}^{57} \\frac{(x_i-\\mu_{i1})^2}{\\sigma_{i1}^2}$$\n",
    "On retrouve la règle R1.    \n",
    "    \n",
    "On peut conclure: la règle de décision R2 (GNB sur les données normalisées) et R1 (GNB sur les données originales) sont identiques.\n",
    "    \n",
    "Néanmoins, la procédure d'estimation des moyennes et écarts-types sont légèrement différentes entre `StandardScaler` et `GaussianNB` (estimateurs empiriques pour le premier, estimation du maximum de vraisemblance pour le second). Cela peut expliquer une légère différence en pratique. Le seul intérêt de la standardisation préalable pour GNB est d'éviter parfois des problèmes d'optimisation numérique.\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"classification par GNB\")\n",
    "GNB = naive_bayes.GaussianNB()  \n",
    "scores_GNB = cross_val_score(GNB,X_train,y_train,cv=5)\n",
    "print(\"score moyen de validation croisée: %0.3f (+/- %0.3f)\" % (scores_GNB.mean(),2*scores_GNB.std()))\n",
    "GNB.fit(X_train,y_train)  \n",
    "y_pred_GNB = GNB.predict(X_test)  \n",
    "score_GNB = GNB.score(X_test,y_test)\n",
    "print(\"score sur la base de test: %0.3f\" %score_GNB)\n",
    "\n",
    "\n",
    "# avec normalisation  (décommentez les lignes suivantes)\n",
    "#print(\"\\nclassification par GNB après normalisation\")\n",
    "#GNB = naive_bayes.GaussianNB()  \n",
    "#scores_GNB = cross_val_score(GNB,X_train_standard,y_train,cv=5)\n",
    "#print(\"score moyen de validation croisée: %0.3f (+/- %0.3f)\" % (scores_GNB.mean(),2*scores_GNB.std()))\n",
    "#GNB.fit(X_train_standard,y_train)  \n",
    "#y_pred_GNB = GNB.predict(X_test_standard)  \n",
    "#score_GNB = GNB.score(X_test_standard,y_test)\n",
    "#print(\"score test: %0.3f\" %score_GNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mettez en oeuvre le classifieur de la régression logistique ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)). \n",
    "\n",
    "Passez l'option `max_iter=2000` si vous avez un avertissement concernant la convergence de l'optimisation, de la manière suivante:\n",
    "`LR = linear_model.LogisticRegression(max_iter=2000)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# classifieur régression logistique (sur les données standardisées)\n",
    "\n",
    "# d'après la documentation: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "# il y a régularisation par défaut\n",
    "# on fait une gridsearch (voir la documentation) pour trouver la valeur optimale de C par validation croisée \n",
    "# Ce n'est pas demandé, on peut se contenter de la valeur de C par défaut (C=1) ou penalty=\"none\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"Régression logistique: gridsearch\\n\")\n",
    "LR = linear_model.LogisticRegression(max_iter=2000)   # il faut augmenter max_iter sinon on a des problèmes de convergence\n",
    "GS=GridSearchCV(LR,{'C':[0.001,0.01,0.1,1,10,100, 1000, 10000, 100000, 1000000]},cv=5)\n",
    "GS.fit(X_train_standard,y_train)\n",
    "print(\"\\nmeilleur estimateur:\")\n",
    "print(GS.best_estimator_)\n",
    "print(\"\\nscore de validation croisée pour chaque valeur de C:\")\n",
    "print(GS.cv_results_['mean_test_score'])\n",
    "plt.figure()\n",
    "plt.semilogx([0.001,0.01,0.1,1,10,100, 1000, 10000, 100000, 1000000],GS.cv_results_['mean_test_score'],'-*')\n",
    "plt.title('score de validation croisée contre C')\n",
    "plt.grid()\n",
    "plt.show();\n",
    "\n",
    "# on demandait uniquement le code suivant:\n",
    "print(\"\\nClassification par régression logistique\")\n",
    "LR = linear_model.LogisticRegression() #(max_iter=2000) à ajouter dans certaines versions de sklearn\n",
    "scores_LR = cross_val_score(LR,X_train_standard,y_train,cv=5)\n",
    "print(\"score moyen de validation croisée: %0.3f (+/- %0.3f)\" % (scores_LR.mean(),2*scores_LR.std()))\n",
    "%time LR.fit(X_train_standard, y_train)\n",
    "score_LR = LR.score(X_test_standard, y_test)\n",
    "print('score: %2f' %score_LR)\n",
    "y_pred_lr = LR.predict(X_test_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "\n",
    "_Remarque sur la validation croisée (gridsearch)_ : à partir de $C=0.1$, le mean_test_score ne change qu'au troisième chiffre après la virgule. On aurait tout aussi bien pu prendre $C=0.1$ (ou $C=1$, valeur par défaut suggérée par l'énoncé), et d'ailleurs on aurait pu avoir un optimum différent avec un autre nombre de pli ou tirage aléatoire des plis.\n",
    "    \n",
    "C'est une remarque valable dans toutes ces questions où on cherche le \"meilleur\" hyperparamètre: il ne faut pas chercher à départager au delà des fluctuations d'échantillonnage.\n",
    "    \n",
    "Encore une fois, le choix de l'hyperparamètre $C$ par validation croisée n'était pas demandé.\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyse des résultats\n",
    "\n",
    "<br>\n",
    "\n",
    "On dispose des matrices de confusion, décrites [ici](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html), et des rapports de classification, décrits [là](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html).\n",
    "\n",
    "__Question 6__. Affichez ces matrices et rapports sur la base test pour les quatre classifieurs étudiés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPlus proche voisin, données standardisées:\")\n",
    "print(\"score sur la base de test: %0.3f\" %score_knn)\n",
    "print(metrics.classification_report(y_test,y_pred_knn))\n",
    "print(\"matrice de confusion:\")\n",
    "print(metrics.confusion_matrix(y_test,y_pred_knn))\n",
    "\n",
    "print(\"\\n5 plus proches voisins, données standardisées:\")\n",
    "print(\"score sur la base de test: %0.3f\" %score_knn5)\n",
    "print(metrics.classification_report(y_test,y_pred_knn5))\n",
    "print(\"matrice de confusion:\")\n",
    "print(metrics.confusion_matrix(y_test,y_pred_knn5))\n",
    "\n",
    "print(\"\\nClassifieur naïf Gaussien:\")\n",
    "print(\"score sur la base de test: %0.3f\" %score_GNB)\n",
    "print(metrics.classification_report(y_test,y_pred_GNB))\n",
    "print(\"matrice de confusion:\")\n",
    "print(metrics.confusion_matrix(y_test,y_pred_GNB))\n",
    "\n",
    "print(\"\\nRégression logistique:\")\n",
    "print('score sur la base de test: %0.3f' %score_LR)\n",
    "print(metrics.classification_report(y_test,y_pred_lr))\n",
    "print(\"matrice de confusion:\")\n",
    "print(metrics.confusion_matrix(y_test,y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 7__. Ici, par quoi pourraient s'expliquer les performances modestes du classifieur naïf de Bayes ? A ce stade, quel classifieur préfére-t-on et pourquoi? Dans une application de détection de spams, cherche-t-on réellement à minimiser le taux d'erreur global?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "  \n",
    "Le classifieur naïf de Bayes suppose l'indépendance conditionnelle des caractéristiques. Cela ne permet pas de modéliser les corrélations entre caractéristiques. Or un spam est caractérisé par la fréquence élevée de plusieurs mots en même temps (co-occurrence). Par exemple, un mail avec plusieurs occurrences simultanées de \"money\", \"receive\", \"viagra\" est suspicieux; il le sera moins s'il contient l'un de ces mots mais pas les autres. Le classifieur naïf de Bayes se base uniquement sur la distribution des fréquences des mots considérés individuellement, pas sur leurs corrélations, ce qui explique les performances modestes.\n",
    "    \n",
    "<br>\n",
    "    \n",
    "__Attention__ à l'ordre des arguments dans les deux fonctions: `y_true`, puis `y_pred` (sinon on récupère par exemple la transposée de la matrice de confusion)\n",
    "    \n",
    "Cf [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html)\n",
    "    \n",
    "* precision = tp / (tp+fp)\n",
    "* recall = tp / (tp+fn)   (rappel)\n",
    "* f1_score: 2 * precision * recall / (precision + recall)  (moyenne harmonique de precision et recall)\n",
    "* support: nombre d'observations classées dans classe k\n",
    "\n",
    "où tp = true positive (classé en $k$ à raison), fp = false positive (classé en $k$ à tort), fn = false negative (classé dans une autre classe à tort).\n",
    " \n",
    "interprétation:  \n",
    "* tp+fp: nombre d'observations classées dans la classe k (à raison + à tort)\n",
    "* tp+fn: nombre d'observations réellement dans la classe k  (bien détectées + mal détectées)\n",
    "* precision = proportion d'observations classées dans k à raison\n",
    "* recall = proportion d'observations classées dans k parmi tous les éléments réellement dans la classe k  \n",
    " \n",
    "Idéalement, on voudrait precision et recall proches de 1 (s'ils sont égaux à 1, fp=fn=0).\n",
    "  \n",
    "<br>\n",
    "\n",
    "En ce qui concerne `confusion_matrix`, d'après la documentation, les vraies classes sont en lignes, les classes prédites en colonnes.\n",
    "    \n",
    "<br>\n",
    "    \n",
    "On voit par exemple que GNB classe 21 mails en non-spam alors qu'ils sont des spams, parmi les 21+336=357 spams. Par contre 148 mails ont été classés en spam alors qu'ils n'en sont pas, parmi les 148+416=564 non-spams.\n",
    "    \n",
    "<br>\n",
    "    \n",
    "Dans cette application, on souhaite ne pas classer à tort des messages \"normaux\" en spam (car ils n'arriveront alors jamais à l'utilisateur). Le critère de choix serait donc le rappel de la classe 0, que l'on veut le plus grand possible, plutôt que le score de classification.\n",
    "    \n",
    "Le classifieur de la régression logistique est donc la \"meilleure\" méthode parmi celles testées selon ce critère: 96% des non-spams sont bien classés. C'est également celle qui a le meilleur score global: dans 92% des cas, sa prédiction est correcte.\n",
    "    \n",
    "Au passage, on voit que GNB a la meilleure précision pour la classe 0: cela signifie qu'il est le meilleur pour détecter les non-spams (lorsqu'il détecte un non-spam, il se trompe peu). Le problème est que lorsqu'il prétend détecter un spam, il se trompe souvent. GNB a également le meilleur rappel pour la classe 1: un vrai spam est bien classé en spam. Ce n'est pas contradictoire avec la phrase précédente: le problème est que les détections en spam sont souvent en fait des non-spams.\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Toutes les erreurs ne se valent pas...\n",
    "\n",
    "Le classifieur bayésien naïf gaussien et le classifieur de la régression logistique s'appuient tous deux sur la règle du maximum a posteriori. Ils permettent d'estimer la probabilité a posteriori $p(C_1|x)$ et détectent un spam lorsque $p(C_1|x)>1/2$, où $C_1$ désigne la classe \"spam\" et $x$ est une observation. Les deux classifieurs mettent en oeuvre le classifieur de Bayes, qui minimise le risque moyen de prédiction (le taux d'erreur). Le taux d'erreur \"compte\" de la même manière les erreurs sur les deux classes.\n",
    "\n",
    "Si on préfère réduire le taux de faux positif de la méthode (proportion de mails détectés à tort comme \"spam\"), on peut relever le seuil de cette probabilité.\n",
    "\n",
    "Les classifieurs `LogisticRegression` et `GaussianNB` possèdent tous deux une méthode `predict_proba` qui, pour un tableau d'observations, fournit la probabilité a posteriori de chaque classe, comme l'affiche la cellule suivante. On remarque que pour chaque observation $x$, $p(C_0|x)+p(C_1|x)=1$.  (attention, la documentation n'est pas très claire, `predict_proba` fournit bien la probabilité a posteriori, et pas la vraisemblance $p(x|C_k)$)\n",
    "\n",
    "Remarquons qu'aucune probabilité n'est fournie par la classification aux plus proches voisins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"probabilités a posteriori pour GNB:\")\n",
    "print(GNB.predict_proba(X_test))\n",
    "print(\"\\nprobabilités a posteriori pour LR:\")\n",
    "print(LR.predict_proba(X_test_standard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faites varier le seuil de détection p:\n",
    "p=0.63  # constatez que p=0.5 fournit les mêmes résultats pour y_pred_lr et y_pred_LRb\n",
    "y_pred_LRb = (LR.predict_proba(X_test_standard)[:,1] >= p).astype(int)\n",
    "#print(y_pred_LRb)   # pour visualiser les classes prédites\n",
    "#print(y_pred_lr)\n",
    "score_LRb = 1-np.mean(np.abs(y_test-y_pred_LRb))  # calcul du taux de reconnaissance\n",
    "\n",
    "print(\"\\nClassification de la régression logistique pour un seuil de probabilité p=%.3f\" %p)\n",
    "print('score sur la base de test: %.3f' %score_LRb)\n",
    "print(metrics.classification_report(y_test,y_pred_LRb))\n",
    "print(\"matrice de confusion:\")\n",
    "print(metrics.confusion_matrix(y_test,y_pred_LRb))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 8__. Quelle valeur du seuil de probabilité $p$ faut-il choisir pour assurer un rappel de la classe \"non spam\" d'au moins 0.98?\n",
    "Que penser de cet algorithme de détection de spam?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "    \n",
    "Il faut choisir $p=0.63$ pour LR dans la cellule précédente: 14 mails classés en spam à tort parmi 550+14 = 564 mails qui ne sont pas du spam.\n",
    "\n",
    "On constate que le taux de reconnaissance (score) décroît par rapport à la classification obtenue avec le seuil $p=0.5$.\n",
    "\n",
    "On a alors un rappel de la classe \"spam\" de 0.83: 62 mails ont été classés en non-spam alors qu'ils le sont parmi les 62+295 spams reçus.\n",
    "\n",
    "<br>\n",
    "    \n",
    "__Conclusion__: lorsqu'on règle le seuil $p$ de l'algorithme de manière à mal classer moins de 2% des mails qui ne sont pas des spams, on détecte correctement un peu plus des trois-quarts des spams reçus. Ce sont des performances modestes. Néanmoins on est tributaire du \"sac de mots\" choisi initialement pour former le fichier sur lequel on travaille, que l'on ne maîtrise pas dans cet exercice.\n",
    "\n",
    "Remarquons qu'il est très simple d'assurer un rappel de la classe \"non spam\" (on parle de sensibilité, _sensitivity_ ) de 1: il suffit de classer tout mail comme non-spam... Bien entendu, le rappel de la classe \"spam\" (on parle de spécificité, _specificity_ ) est alors nul. Toute méthode de détection doit être analysée simultanément selon ces deux critères, le seuil de détection devant être choisi selon un compromis entre sensibilité et spécificité.\n",
    "\n",
    "__Pour aller plus loin__, lire ce qui concerne les __[courbes ROC](https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics)__.\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
