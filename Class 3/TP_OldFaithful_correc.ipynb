{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction à l'apprentissage automatique: TP additionnel - <font color=red> CORRECTION </font>\n",
    "\n",
    "<br>\n",
    "\n",
    "### Mélanges de gaussiennes et jeu de données \"Old Faithful\", prédiction par Gaussian Mixture Regression\n",
    "\n",
    "<br>\n",
    "On va étudier un jeu de données relatif au geyser \"Old Faithful\" dans le parc du Yellowstone aux Etats-Unis.\n",
    "\n",
    "Chaque observation est constituée de deux caractéristiques: la durée d'une éruption et la durée avant l'éruption suivante.\n",
    "\n",
    "\n",
    "Les données sont décrites ici: \n",
    "http://www.stat.cmu.edu/~larry/all-of-statistics/=data/faithful.dat <br>\n",
    "et doivent être téléchargées sur Arche ou [sur la page web du cours](https://members.loria.fr/FSur/enseignement/apprauto/old_faithful.txt).\n",
    "\n",
    "<br>\n",
    "On commence par charger les bibliothèques qui nous seront utiles:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from sklearn import mixture\n",
    "\n",
    "%matplotlib notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "puis on charge les données du fichier `old_faithful.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention: le notebook doit être ouvert dans le même répertoire que le fichier old_faithful.txt\n",
    "data=np.loadtxt('old_faithful.txt')\n",
    "print(data)  # pour vérifier que les données sont bien chargées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par représenter les données par un nuage de point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,10]);\n",
    "plt.scatter(data[:,1], data[:,2]);\n",
    "plt.grid()\n",
    "plt.xlabel('durée éruption')\n",
    "plt.ylabel('temps avant la prochaine éruption')\n",
    "plt.title('Old Faithful');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modélisation d'une densité de probabilité par mélange de gaussiennes\n",
    "\n",
    "Les observations sont des couples $(x,y)$ où $x$ est la durée d'éruption et $y$ le temps avant la prochaine éruption. Nous allons modéliser la loi jointe du couple $(x,y)$ à l'aide d'un mélange de gaussiennes.\n",
    "\n",
    "__Question 1__. Observez l'évolution du critère BIC dans l'ajustement d'un mélange de gaussiennes à $M$ composantes, pour $M$ variant entre 1 et 10 (inspirez-vous du code ci-dessous, et utilisez la [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)). Vous afficherez les lignes d'iso-valeur du logarithme de la densité de probabilité estimée pour chaque valeur de $M$ en utilisant la fonction `show_mixture` ci-dessous (inspirée du code de la [documentation scikit-learn](http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_pdf.html)).\n",
    "\n",
    "Constatez que l'évolution du critère BIC semble justifier que ce geyser présente deux \"modes de fonctionnement\". \n",
    "\n",
    "<br>\n",
    "\n",
    "_Remarque 1_ : $M$ est appelé un _hyperparamètre_ du modèle car il doit être fixé par l'utilisateur, à l'aide d'indicateurs comme BIC ici. Les paramètres du modèle sont les poids, moyennes, et matrices de covariance: ils sont estimés à partir des données par la fonction `fit` ci-dessous (qui, ici, met en oeuvre l'algorithme EM).\n",
    "\n",
    "_Remarque 2_ : par définition, la log-vraisemblance calculée à partir d'une observation (ce qui est retourné par `score_samples` ci-dessous) est le logarithme de la valeur de la densité de probabilité en cette observation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mixture(mixture,X_train):\n",
    "    # mixture: objet GaussianMixture ajusté à des données\n",
    "    # X_train: pour le scatter-plot, observations sous forme d'un tableau (observations en lignes, caractéristiques en colonnes)\n",
    "    x = np.linspace(1, 5.5)\n",
    "    y = np.linspace(20., 120.)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    XX = np.array([X.ravel(), Y.ravel()]).T\n",
    "    Z = -mixture.score_samples(XX)  # score_samples est le log de la densité (log-vraisemblance) en chaque point  \n",
    "    Z = Z.reshape(X.shape)\n",
    "    plt.figure(figsize=[8,6])\n",
    "    CS = plt.contour(X, Y, Z, norm=LogNorm(vmin=1.0, vmax=100.0),\n",
    "                    levels=np.logspace(0, 3, 10))   # lignes d'iso-valeur\n",
    "    CB = plt.colorbar(CS, shrink=0.8, extend='both')\n",
    "    plt.scatter(X_train[:, 0], X_train[:, 1])  # nuage des observations\n",
    "    plt.scatter(mixture.means_[:,0],mixture.means_[:,1],c='red')  # moyennes des composantes en rouge\n",
    "    plt.title('Level lines of negative log-likelihood predicted by a GMM, K='+str(mixture.n_components))\n",
    "    plt.axis('tight')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# je ne demande que le cas \"full\", mais je mets le code pour essayer les autres modèles\n",
    "# (voir la documentation de GaussianMixture pour la signification des types de covariance)\n",
    "\n",
    "print(\"Full\\n\")\n",
    "BIC_full=[]\n",
    "for K in range(1,11):\n",
    "    GMM=mixture.GaussianMixture(n_components=K,covariance_type='full')\n",
    "    GMM.fit(data[:,1:3])\n",
    "    BIC_full.append(GMM.bic(data[:,1:3]))\n",
    "    print('K: %d  BIC=%.2f' %(K,BIC_full[-1]))\n",
    "    show_mixture(GMM,data[:,1:3])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1,11),BIC_full)\n",
    "plt.legend((\"full\"))          \n",
    "plt.xlabel('K')\n",
    "plt.ylabel('BIC')\n",
    "plt.grid()\n",
    "plt.show();\n",
    "\n",
    "print(\"Tied\\n\")\n",
    "BIC_tied=[]\n",
    "for K in range(1,11):\n",
    "    GMM=mixture.GaussianMixture(n_components=K,covariance_type='tied')\n",
    "    GMM.fit(data[:,1:3])   \n",
    "    BIC_tied.append(GMM.bic(data[:,1:3]))\n",
    "    print('K: %d  BIC=%.2f' %(K,BIC_tied[-1]))\n",
    "    show_mixture(GMM,data[:,1:3])\n",
    "    \n",
    "print(\"Diag\\n\")\n",
    "BIC_diag=[]\n",
    "for K in range(1,11):\n",
    "    GMM=mixture.GaussianMixture(n_components=K,covariance_type='diag')\n",
    "    GMM.fit(data[:,1:3])   \n",
    "    BIC_diag.append(GMM.bic(data[:,1:3]))\n",
    "    print('K: %d  BIC=%.2f' %(K,BIC_diag[-1]))\n",
    "    show_mixture(GMM,data[:,1:3])\n",
    "          \n",
    "print(\"Spherical\\n\")\n",
    "BIC_sphe=[]\n",
    "for K in range(1,11):\n",
    "    GMM=mixture.GaussianMixture(n_components=K,covariance_type='spherical')\n",
    "    GMM.fit(data[:,1:3])   \n",
    "    BIC_sphe.append(GMM.bic(data[:,1:3]))\n",
    "    print('K: %d  BIC=%.2f' %(K,BIC_sphe[-1]))\n",
    "    show_mixture(GMM,data[:,1:3])\n",
    "          \n",
    "plt.figure()\n",
    "plt.plot(range(1,11),BIC_full)\n",
    "plt.plot(range(1,11),BIC_tied)\n",
    "plt.plot(range(1,11),BIC_diag)\n",
    "plt.plot(range(1,11),BIC_sphe)\n",
    "plt.legend((\"full\",\"tied\",\"diag\",\"spherical\"))          \n",
    "plt.xlabel('K')\n",
    "plt.ylabel('BIC')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> \n",
    "    \n",
    "Le dernier graphe nous montre que $K=2$ est bien minimum du critère BIC\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2__. Ecrivez les paramètres (poids, moyennes, matrices de covariance) du mélange de gaussienne optimal au sens de BIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMM = mixture.GaussianMixture(n_components=2)\n",
    "GMM.fit(data[:,1:3])  \n",
    "print(\"BIC: %.3f\" %GMM.bic(data[:,1:3])) \n",
    "\n",
    "print(\"\\nweights:\")  # on vérifie que la somme des poids est 1\n",
    "print(GMM.weights_)\n",
    "\n",
    "print(\"\\nmeans:\")\n",
    "print(GMM.means_)\n",
    "\n",
    "print(\"\\ncovariances:\")\n",
    "print(GMM.covariances_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "    \n",
    "Densité estimée:\n",
    "$$ p(x,y) = \\pi_1 {\\cal G}_{\\mu_1,\\Sigma_1} (x,y) + \\pi_2 {\\cal G}_{\\mu_2,\\Sigma_2} (x,y)$$  \n",
    "avec les notations de la section 4.2.2.2 du polycopié.\n",
    "\n",
    "Les moyennes sont les points rouges dans les graphiques de `show_mixture`. Les moyennes et matrices de covariance ont des grandes valeurs selon $y$ car cette variable varie dans un intervalle beaucoup plus grand que $x$.\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 3__. La \"méthode du coude\" de $K$-Means permet-elle de justifier un classification en deux groupes ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voir le code de l'exercice 1 \n",
    "from sklearn import cluster\n",
    "\n",
    "inertie=np.zeros((10))\n",
    "for K in range(1,11):\n",
    "    clustering=cluster.KMeans(n_clusters=K)\n",
    "    clustering.fit(data[:,1:3])\n",
    "    inertie[K-1]=clustering.inertia_\n",
    "plt.figure();\n",
    "plt.plot(np.arange(1,11),inertie);\n",
    "plt.xlabel(\"K\");\n",
    "plt.ylabel(\"inertie\");\n",
    "plt.title(\"elbow plot\");\n",
    "\n",
    "clustering=cluster.KMeans(n_clusters=2)\n",
    "clustering.fit(data[:,1:3])\n",
    "plt.figure()\n",
    "plt.scatter(data[:, 1], data[:, 2], s=10,c=clustering.labels_)\n",
    "plt.title('Kmeans, n_cluster=2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "    \n",
    "Oui: on voit un inflexion assez franche dans le elbow plot pour $K=2$.\n",
    "    \n",
    "Ici, les groupes peuvent être identifiés avec $K$-means, on peut bien utiliser la méthode du coude.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration de la théorie développée au chapitre 4, section 4.1.1\n",
    "\n",
    "## Prédiction du temps avant la prochaine éruption par Gaussian mixture regression\n",
    "\n",
    "Nous allons à présent prédire le temps d'attente avant la prochaine éruption en fonction de la durée d'une éruption.\n",
    "\n",
    "Si $x$ désigne la durée d'une éruption et $y$ le temps avant la prochaine éruption, nous venons de modéliser la densité $p(x,y)$ de la loi de probabilité jointe de $x$ et $y$ comme un mélange de deux gaussiennes.\n",
    "\n",
    "Nous verrons lors de la prochaine séance (polycopié chapitre 4, section 4.1.1) que la meilleure prédiction (en un sens précis) de $y$ en fonction de $x$ est donnée par l'espérance conditionnelle suivante:\n",
    "$$ h(x) = E(y|x) = \\int_y y p(y|x) dy$$\n",
    "Il s'agit de l'espérance de $y$ connaissant $x$. Autrement dit, il s'agit de la valeur moyenne de $y$ à $x$ fixé.\n",
    "\n",
    "Par définition,\n",
    "$$p(y|x) = \\frac{p(x,y)}{p(x)}$$\n",
    "et la densité marginale $p(x)$ est déterminée par:\n",
    "$$ p(x) = \\int_y p(x,y) dy$$\n",
    "Malheureusement, `GaussianMixture` de scikit-learn n'implémente pas ces calculs. Nous allons donc déterminer l'espérance conditionnelle $h(x)$ par intégration numérique, à partir du mélange de gaussiennes $p(x,y)$.\n",
    "\n",
    "C'est ce que fait la fonction `GMM_regression` dans la cellule suivante: ses arguments sont 1) un objet `GaussianMixture` représentant un mélange de Gaussiennes, et 2) un tableau de données $x$; elle retourne le tableau des valeurs prédites pour les valeurs de $x$ fournies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_regression(GMM,x):\n",
    "    y = np.linspace(20., 120.,num=1001)  # y discrétisé entre 20 et 120 par pas de 0.1 (1001 valeurs)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    XX = np.array([X.ravel(), Y.ravel()]).T\n",
    "    p = np.exp(GMM.score_samples(XX))   # score_samples fournit la log-vraisemblance d'une observation, p est donc bien la valeur de la densité  \n",
    "    p = p.reshape(X.shape)  # on construit ainsi la carte des p(x,y)\n",
    "    condexp = np.sum(Y*p,axis=0)/np.sum(p,axis=0)  # espérance conditionnelle obtenue par intégration numérique (ici, le première axe correspond à y)\n",
    "    return condexp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 4__. Complétez le code suivant de manière à tracer le graphique de la prédiction du temps entre éruptions en fonction de la durée d'éruption.\n",
    "\n",
    "Comparez votre prédiction avec ce qui est proposé sur __[cette page](https://www.nps.gov/teachers/classrooms/predict-old-faithful.htm)__ du National Park Service. (nos données sont un peu ancienne, le \"fonctionnement\" du geyser évolue au fil des années)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMM=mixture.GaussianMixture(n_components=2,covariance_type='full')\n",
    "GMM.fit(data[:,1:3])\n",
    "x = np.linspace(1, 5.5)\n",
    "\n",
    "condexp = GMM_regression(GMM,x)\n",
    "\n",
    "plt.figure(figsize=[12,10])\n",
    "plt.scatter(data[:,1], data[:,2]);\n",
    "plt.plot(x,condexp,'r')\n",
    "plt.grid()\n",
    "plt.xlabel('durée éruption')\n",
    "plt.ylabel('temps entre éruptions')\n",
    "plt.title('Old Faithful')\n",
    "plt.legend((\"GMM regression\",'data'))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "    \n",
    "Notre prédiction identifie bien un changement de régime à partir de $x\\simeq 3$ minutes.\n",
    "    \n",
    "L'espérance conditionnelle $E(y|x)$ dans le cas d'une loi jointe gaussienne correspond exactement à la régression linéaire (voir aussi le cours d'analyse de données). Ceci explique l'aspect de la courbe de régression: pour x franchement inférieur à 3 ou franchement supérieur à 3, la loi jointe $p(x,y)$ est dominée par l'une ou l'autre des deux gaussiennes, l'autre ayant une contribution très faible. Dans chaque cas, la courbe de régression est donc très proche d'un segment de droite.\n",
    "    \n",
    "Cette __[vidéo](https://www.nps.gov/yell/learn/photosmultimedia/indepth-predictingoldfaithful.htm)__ suggère que d'autres éléments sont pris en compte dans la prédiction par les rangers du Yellowstone Park.\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 5__. Tracez la droite de régression linéaire superposée à l'estimation par espérance conditionnelle dans laquelle la loi jointe $p(x,y)$ est supposée gaussienne (il suffit de faire `n_components=1` dans `GaussianMixture`), et constatez que les prédictions sont identiques dans les deux cas. La raison est expliquée dans la section consacrée à la régression dans le polycopié."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm\n",
    "GMM=mixture.GaussianMixture(n_components=1,covariance_type='full')\n",
    "GMM.fit(data[:,1:3])\n",
    "x = np.linspace(1, 5.5)\n",
    "\n",
    "linear_reg=lm.LinearRegression()\n",
    "linear_reg.fit(data[:,1].reshape(-1, 1),data[:,2].reshape(-1, 1))\n",
    "\n",
    "condexp = GMM_regression(GMM,x)\n",
    "plt.figure(figsize=[12,10])\n",
    "\n",
    "plt.scatter(data[:,1], data[:,2]);\n",
    "x = np.linspace(1, 5.5)\n",
    "plt.plot(x,condexp,'r')\n",
    "plt.plot(x,linear_reg.predict(x.reshape(-1,1)),'g*')\n",
    "plt.grid()\n",
    "plt.xlabel('durée éruption')\n",
    "plt.ylabel('temps entre éruptions')\n",
    "plt.title('Old Faithful')\n",
    "plt.legend(('GMM regression','lm','data'))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "    \n",
    "Par contre l'estimation des paramètres ne se fait pas de la même manière dans les deux approches: EM pour GMM, équations normales pour régression linéaire. Cela explique les légères différences.\n",
    "   \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
