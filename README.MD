# Orgonograma geral de Aprendisagem de MÃ¡quina

## Aprendisagem nÃ£o supervisionada:

### K-means

#### Algoritmo de Floyd:

1. Fixar os centros iniciais.
2. Calcular a matriz de distÃ¢ncias em relaÃ§Ã£o Ã  esses centros iniciais.
3. Setar k
4. Calcular o novo centro.
5. Repetir.

#### InÃ©rcia:

No contexto do k-means, a inÃ©rcia Ã© uma medida que representa a soma das distÃ¢ncias ao quadrado entre os pontos dos dados e os centroides (mÃ©dias) dos grupos aos quais esses pontos foram atribuÃ­dos. Em termos formais:

![alt text](image-6.png)

#### MÃ©todo do Cotovelo (Elbow Method)

A inÃ©rcia mÃ­nima Ã© frequentemente usada para determinar o nÃºmero ideal de clusters em um conjunto de dados.
No mÃ©todo do cotovelo, traÃ§a-se a inÃ©rcia mÃ­nima em funÃ§Ã£o de diferentes valores de ğ‘˜.
O ponto onde a inÃ©rcia comeÃ§a a diminuir de forma mais lenta (formando um "cotovelo" no grÃ¡fico) Ã© escolhido como o nÃºmero ideal de clusters.

### ClassificaÃ§Ã£o HierÃ¡rquica

organizar e classificar dados em uma estrutura hierÃ¡rquica, onde as classes ou categorias estÃ£o dispostas em diferentes nÃ­veis de especificidade. Essa tÃ©cnica Ã© especialmente Ãºtil em problemas onde as classes tÃªm uma relaÃ§Ã£o hierÃ¡rquica natural, como taxonomias biolÃ³gicas, categorias de produtos, ou classificaÃ§Ã£o de documentos.

## Aprendisagem Supervisionada:

### Modelos Linear

#### RegressÃ£o Ridge

Ele endereÃ§a alguns erros de Ordem MÃ­nima quadrÃ¡tica impondo uma penalidade no tamanho dos coeficientes.
![alt text](image-12.png)

##### RegressÃ£o Lasso

Reduz o nÃºmero de features com a qual a soluÃ§Ã£o Ã© dependente. Adiciona um fator de normalizaÃ§Ã£o inversamente proprocional ao dobro da amostra.

### ClassificaÃ§Ã£o HierÃ¡rquica

#### Single Linkage (LigaÃ§Ã£o Simples)

A distÃ¢ncia entre dois clusters Ã© definida como a menor distÃ¢ncia entre qualquer ponto de um cluster e qualquer ponto do outro cluster.
Quando vocÃª deseja clusters que conectem pontos com base em proximidade direta.

#### Complete Linkage (LigaÃ§Ã£o Completa)

A distÃ¢ncia entre dois clusters Ã© definida como a maior distÃ¢ncia entre qualquer ponto de um cluster e qualquer ponto do outro cluster.

#### Average Linkage (LigaÃ§Ã£o MÃ©dia)

A distÃ¢ncia entre dois clusters Ã© definida como a mÃ©dia das distÃ¢ncias entre todos os pares de pontos, sendo um ponto de cada cluster.

#### Ward's Method (CritÃ©rio de Ward)

Minimiza o aumento da variÃ¢ncia interna ao combinar clusters.
A distÃ¢ncia entre clusters Ã© baseada no aumento da soma dos quadrados das distÃ¢ncias dos pontos ao centroide.

### Perceptron (Supervisionado)

Classifieur Lineaire. Se os dados nÃ£o sÃ£o separÃ¡veis o algoritmo nÃ£o converge.
O Perceptron tenta apenas encontrar um hiperplano de separaÃ§Ã£o qualquer que consiga dividir os dados linearmente separÃ¡veis.
Ele ajusta os pesos iterativamente corrigindo erros de classificaÃ§Ã£o, sem se preocupar com a "qualidade" do hiperplano escolhido.
NÃ£o possui uma funÃ§Ã£o objetivo global para ser minimizada (apenas segue uma regra de atualizaÃ§Ã£o).

NÃ£o posssui regularizaÃ§Ã£o.

Tanto o Perceptron quanto as redes neurais usam combinaÃ§Ãµes lineares de entradas (com pesos e bias) e aplicam funÃ§Ãµes de ativaÃ§Ã£o para produzir uma saÃ­da.

### Redes Neurais

Um Perceptron simples Ã© o componente fundamental de uma rede neural. Redes neurais podem ser vistas como uma composiÃ§Ã£o de Perceptrons multicouches (multicamadas), organizados em camadas (input, hidden e output).

O nÃºmero de **parÃ¢metros** no modelo (d+1)n + (n+1)s, onde d sÃ£o os neuronios de entrada, n neurÃ´nios na camada escondida e s neurÃ´nios na camada de saÃ­da.

### K-NN

1. Calcular a distÃ¢ncia do ponto aos outros. (euclidiana ou manhatan)
2. Ordenar pelos vizinhos mais prÃ³ximos do ponto
3. Para o problema de classificaÃ§Ã£o, ele Ã© classificado de acordo com o maioria dos seus vizinhos.
   ![alt text](image.png)

#### Score Leave-One-Out

O leave-one-out cross-validation Ã© uma tÃ©cnica para avaliar um classificador:

Cada vez, retiramos uma Ãºnica observaÃ§Ã£o da base de treinamento e usamos o restante para treinar o modelo.

Classification: Cada observaÃ§Ã£o aparenta Ã  uma classe.

Regression: Cada observaÃ§Ã£o estÃ¡ associada Ã  uma grandeza escalar ou vetorial.

### SVM (MÃ¡quinas de Suporte) Supervisionado

O SVM, diferentemente do Perceptron, busca um hiperplano Ã³timo, que maximize a margem entre as classes (a distÃ¢ncia mÃ­nima entre o hiperplano e os pontos de cada classe).

#### A astÃºcia do kernel

A relaÃ§Ã£o entre uma funÃ§Ã£o de plongement Ï†(x) e um nÃºcleo k(x, z) baseia-se no **produto interno** no espaÃ§o de caracterÃ­sticas. O nÃºcleo k(x, z) Ã© uma forma compacta de calcular este produto interno, sem necessidade de computar explicitamente a transformaÃ§Ã£o Ï†(x).

Este documento apresenta um **passo a passo** para determinar o nÃºcleo correspondente a uma transformaÃ§Ã£o Ï†(x), como no exemplo fornecido:

Ï†(x1, x2) = (x1Â² - x2Â², x1x2, x1Â² + x2Â²)

**1. Definir o Produto Interno no EspaÃ§o de CaracterÃ­sticas**

O nÃºcleo k(x, z) Ã© definido como o produto interno no espaÃ§o transformado:

k(x, z) = âŸ¨Ï†(x), Ï†(z)âŸ©

Isso significa que, dado Ï†(x) e Ï†(z), vocÃª precisa calcular:

âŸ¨Ï†(x), Ï†(z)âŸ© = âˆ‘ Ï†i(x) Ï†i(z)

Onde Ï†i(x) representa os componentes da transformaÃ§Ã£o Ï†(x).

**2. Substituir Ï†(x) e Ï†(z)**

Escreva explicitamente a transformaÃ§Ã£o fornecida. No caso da questÃ£o, a transformaÃ§Ã£o Ã©:

Ï†(x1, x2) = (x1Â² - x2Â², x1x2, x1Â² + x2Â²)

Agora, substituÃ­mos dois vetores (x1, x2) e (z1, z2) na definiÃ§Ã£o do produto interno:

âŸ¨Ï†(x), Ï†(z)âŸ© = (x1Â² - x2Â²)(z1Â² - z2Â²) + (x1x2)(z1z2) + (x1Â² + x2Â²)(z1Â² + z2Â²)

**3. Expandir os Termos**

Expanda os termos do produto interno:

(x1Â² - x2Â²)(z1Â² - z2Â²) = x1Â²z1Â² - x1Â²z2Â² - x2Â²z1Â² + x2Â²z2Â²

(x1Â² + x2Â²)(z1Â² + z2Â²) = x1Â²z1Â² + x1Â²z2Â² + x2Â²z1Â² + x2Â²z2Â²

Somando todos os termos:

âŸ¨Ï†(x), Ï†(z)âŸ© = 2x1Â²z1Â² + 2x2Â²z2Â² + 2x1x2z1z2

**4. Reconhecer a Forma do NÃºcleo**

Observe que o resultado acima Ã© equivalente ao desenvolvimento de:

(x1z1 + x2z2)Â²

Isso Ã© um **nÃºcleo polinomial de grau 2**.

#### CondiÃ§Ã£o de Mercer

Os nÃºcleos (kernels) sÃ£o usados para transformar dados em um espaÃ§o de alta dimensÃ£o onde problemas nÃ£o linearmente separÃ¡veis podem se tornar linearmente separÃ¡veis.

Um kernel Ã© uma funÃ§Ã£o que calcula um produto interno em um espaÃ§o transformado (espaÃ§o de caracterÃ­sticas), sem a necessidade de calcular explicitamente as transformaÃ§Ãµes.
ğ¾(ğ‘¥,ğ‘§)=ğœ™(ğ‘¥)â‹…ğœ™(ğ‘§)

Para que K(x,z) seja vÃ¡lido, ele deve respeitar a condiÃ§Ã£o de Mercer.

![alt text](image-11.png)

## MalediÃ§Ã£o da DimensÃ£o

O volume da esfera unidade em ğ‘…^ğ‘‘ decai rapidamente com o aumento de ğ‘‘, enquanto o volume do cubo circunscrito cresce exponencialmente. Isso implica que, em alta dimensÃ£o, os pontos tendem a se concentrar nas extremidades ou "cantos" do espaÃ§o, reduzindo a eficÃ¡cia de mÃ©todos baseados em distÃ¢ncia.

# DefiniÃ§Ãµes

## Matriz de ConfusÃ£o

A matriz de confusÃ£o Ã© uma ferramenta usada para avaliar o desempenho de um modelo de classificaÃ§Ã£o. Ela mostra como as previsÃµes do modelo se comparam aos valores reais, organizando os dados em uma tabela que resume as prediÃ§Ãµes corretas e incorretas de cada classe.
Diagonal principal: Valores onde o modelo previu corretamente a classe
Valores fora da diagonal: Erros de classificaÃ§Ã£o (exemplo: a classe 0 foi erroneamente classificada como classe 1, 2 vezes).

Em suma, o valor em cada cÃ©lula `(i, j)` mostra quantas vezes uma observaÃ§Ã£o da classe real i foi prevista como pertencendo Ã  classe j.

|                   | Previsto Classe 0 | Previsto Classe 1 | Previsto Classe 2 | Previsto Classe 3 | Previsto Classe 4 | Previsto Classe 5 |
| ----------------- | ----------------- | ----------------- | ----------------- | ----------------- | ----------------- | ----------------- |
| **Real Classe 0** | 50                | 2                 | 1                 | 0                 | 3                 | 0                 |
| **Real Classe 1** | 4                 | 45                | 2                 | 3                 | 1                 | 0                 |
| **Real Classe 2** | 2                 | 3                 | 47                | 2                 | 1                 | 1                 |
| **Real Classe 3** | 0                 | 2                 | 4                 | 48                | 0                 | 1                 |
| **Real Classe 4** | 3                 | 1                 | 0                 | 0                 | 49                | 2                 |
| **Real Classe 5** | 0                 | 0                 | 3                 | 2                 | 1                 | 44                |

Por exemplo na linha 1: A classe real Ã© Classe 0.
O modelo:

- Previu 50 vezes corretamente como Classe 0.
- Previu 2 vezes erroneamente como Classe 1.
- Previu 1 vez erroneamente como Classe 2.
- E assim por diante.

## FunÃ§Ã£o de Custo

Ela quantifica a diferenÃ§a entre as previsÃµes do modelo e os valores reais esperados (dados rotulados no caso supervisionado).
Exemplos de funÃ§Ãµes de custo: Erro quadrÃ¡tico mÃ©dio (MSE).

## Gradiente de Descida

O gradiente de descida Ã© um algoritmo de otimizaÃ§Ã£o que ajusta iterativamente os parÃ¢metros do modelo para minimizar a funÃ§Ã£o de custo.

Ele usa o gradiente, que Ã© o vetor de derivadas parciais da funÃ§Ã£o de custo em relaÃ§Ã£o aos parÃ¢metros do modelo (ğ‘¤), para indicar a direÃ§Ã£o do aumento mais rÃ¡pido da funÃ§Ã£o de custo.

Em vez de seguir essa direÃ§Ã£o, o gradiente de descida move-se no sentido oposto (descendo), reduzindo a funÃ§Ã£o de custo.

![alt text](image-1.png)

Movendo-se no sentido oposto ao gradiente (âˆ’âˆ‡ğ½(ğ‘¤)), a funÃ§Ã£o de custo diminui.

## Backpropagation

A retropropagaÃ§Ã£o Ã© o algoritmo usado para determinar como um Ãºnico exemplo de treinamento pode ajustar **(nudge)** os pesos e vieses (biases) de uma rede neural. Ela nÃ£o apenas indica se esses valores devem aumentar ou diminuir, mas tambÃ©m em qual **proporÃ§Ã£o relativa** essas mudanÃ§as devem ocorrer para reduzir o custo de maneira mais rÃ¡pida. Em um passo ideal de descida do gradiente, seria necessÃ¡rio calcular isso para todos os milhares de exemplos de treinamento e, em seguida, calcular a mÃ©dia das mudanÃ§as desejadas. No entanto, isso Ã© computacionalmente lento. Em vez disso, os dados sÃ£o divididos aleatoriamente em pequenos lotes **(mini-batches)** e cada passo Ã© calculado com base em um desses lotes.

![alt text](image-2.png)

## Entropia cruzada

Ã‰ a funÃ§Ã£o de perda mais adequada para problemas de classificaÃ§Ã£o multiclasses, pois avalia a diferenÃ§a entre as probabilidades previstas pelo modelo e as classes verdadeiras (usando rÃ³tulos codificados como "one-hot").
